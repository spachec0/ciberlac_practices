Question,Answer
List top 10 LLM Vulnerabilities,LLM01: Prompt Injection LLM02: Sensitive Information Disclosure LLM03: Supply Chain Vulnerabilities LLM04: Data and Model Poisoning LLM05: Improper Output Handling LLM06: Excessive Agency LLM07: System Prompt Leakage LLM08: Vector and Embedding Weaknesses LLM09: Misinformation LLM10: Unbounded Consumption
What is prompt injection in LLM applications?,"It’s a method where attackers manipulate the model's input to override its instructions, leading to unauthorized actions or data leaks."
How can prompt injection lead to privilege escalation?,"If the LLM has elevated privileges and the prompt is tampered with, attackers can execute commands with higher access."
What are common types of prompt injection attacks?,"Direct injection, indirect injection, and jailbreaks that modify the LLM’s behavior or bypass restrictions."
How can sensitive information be exposed by LLMs?,"Through poor data sanitization, inadvertent memorization of training data, or user input leaks."
What is an example of LLM leaking personal data?,A model revealing a previous user’s input like a name or ID due to insufficient input sanitization.
What is model poisoning in LLM security?,"It’s when malicious or biased data is injected during training or fine-tuning, altering model behavior."
How do poisoning attacks affect model integrity?,"They degrade performance, introduce backdoors, or cause biased and harmful outputs."
What is a supply chain threat in LLMs?,"Risks from third-party models, libraries, or tools that may be compromised with malware or backdoors."
Give an example of a model supply chain attack.,Using a Hugging Face-hosted model that was tampered with to include malicious behavior.
How can LLM outputs cause improper output handling issues?,"When outputs are fed directly to systems (e.g., eval, shell) without validation, leading to RCE or injection attacks."
What are the consequences of improper output handling?,"Cross-site scripting (XSS), SQL injection, or execution of malicious code."
How to mitigate prompt injection attacks in LLMs?,"Use strong input validation, escape sequences, and sanitize user inputs."
Why is backdoor insertion via model poisoning dangerous?,"It allows the model to act maliciously when triggered, bypassing regular tests."
What is 'Split-View Data Poisoning'?,"A technique where training data is manipulated so the model sees different realities, causing biased outputs."
What is the role of federated learning in LLM safety?,"It decentralizes training, reducing risks of centralized data exposure."
How can LLMs unintentionally expose business secrets?,Through repeated prompts where sensitive outputs were memorized and reused.
What is an indirect prompt injection?,When data the model processes (like from a plugin or user input) includes hidden instructions.
What are mitigation strategies for output sanitization?,"Escape HTML/JavaScript, use parameterized queries, and apply output encoding."
How does OWASP recommend handling training data leaks?,"With differential privacy, opt-out policies, and strict data governance."
What happens when you use LLM output directly in file paths?,It may cause path traversal vulnerabilities if not sanitized properly.
